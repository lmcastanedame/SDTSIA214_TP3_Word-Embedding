{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb9af7be",
   "metadata": {},
   "source": [
    "# TP : Word Embeddings for Classification\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "Explore the various way to represent textual data by applying them to a relatively small French classification dataset based on professionnal certification titles - **RNCP** - and evaluate how they perform on the classification task. \n",
    "1. Using what we have previously seen, pre-process the data: clean it, obtain an appropriate vocabulary.\n",
    "2. Obtain representations: any that will allow us to obtain a vector representation of each document is appropriate.\n",
    "    - Symbolic: **BoW, TF-IDF**\n",
    "    - Dense document representations: via **Topic Modeling: LSA, LDA**\n",
    "    - Dense word representations: **SVD-reduced PPMI, Word2vec, GloVe**\n",
    "        - For these, you will need to implement a **function aggregating word representations into document representations**\n",
    "3. Perform classification: we can make things simple and only use a **logistic regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7af15",
   "metadata": {},
   "source": [
    "## Necessary dependancies\n",
    "\n",
    "We will need the following packages:\n",
    "- The Machine Learning API Scikit-learn : http://scikit-learn.org/stable/install.html\n",
    "- The Natural Language Toolkit : http://www.nltk.org/install.html\n",
    "- Gensim: https://radimrehurek.com/gensim/\n",
    "\n",
    "These are available with Anaconda: https://anaconda.org/anaconda/nltk and https://anaconda.org/anaconda/scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "079ef84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as op\n",
    "import re \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import gzip\n",
    "pp = pprint.PrettyPrinter(indent=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a147280",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Let's load the data: take a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a5e3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Categorie                                text_certifications\n",
      "0          1  Responsable de chantiers de bûcheronnage manue...\n",
      "1          1  Responsable de chantiers de bûcheronnage manue...\n",
      "2          1                                 Travaux forestiers\n",
      "3          1                                              Forêt\n",
      "4          1                                              Forêt\n"
     ]
    }
   ],
   "source": [
    "with open(\"rncp.csv\", encoding='utf-8') as f:\n",
    "    rncp = pd.read_csv(f, na_filter=False)\n",
    "\n",
    "print(rncp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b21e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Categorie' 'text_certifications']\n"
     ]
    }
   ],
   "source": [
    "print(rncp.columns.values)\n",
    "texts = rncp.loc[:,'text_certifications'].astype('str').tolist()\n",
    "labels = rncp.loc[:,'Categorie'].astype('str').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266fa04f",
   "metadata": {},
   "source": [
    "You can see that the first column is the category, the second the title of the certification. Let's get the category names for clarity: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9293f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "Categories = [\"1-environnement\",\n",
    "              \"2-defense\",\n",
    "              \"3-patrimoine\",\n",
    "              \"4-economie\",\n",
    "              \"5-recherche\",\n",
    "              \"6-nautisme\",\n",
    "              \"7-aeronautique\",\n",
    "              \"8-securite\",\n",
    "              \"9-multimedia\",\n",
    "              \"10-humanitaire\",\n",
    "              \"11-nucleaire\",\n",
    "              \"12-enfance\",\n",
    "              \"13-saisonnier\",\n",
    "              \"14-assistance\",\n",
    "              \"15-sport\",\n",
    "              \"16-ingenierie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2abb70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  'Responsable de chantiers de bûcheronnage manuel et de débardage',\n",
      "   'Responsable de chantiers de bûcheronnage manuel et de sylviculture',\n",
      "   'Travaux forestiers',\n",
      "   'Forêt',\n",
      "   'Forêt',\n",
      "   'Responsable de chantiers forestiers',\n",
      "   'Diagnostic et taille des arbres',\n",
      "   'option Chef d’entreprise ou OHQ en travaux forestiers, spécialité '\n",
      "   'abattage-façonnage',\n",
      "   'option Chef d’entreprise ou OHQ en travaux forestiers, spécialité '\n",
      "   'débardage',\n",
      "   'Gestion et conduite de chantiers forestiers']\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(texts[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9599792e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 94312\n"
     ]
    }
   ],
   "source": [
    "# This number of documents may be high for some computers: we can select a fraction of them (here, one in k)\n",
    "# Use an even number to keep the same number of positive and negative reviews\n",
    "k = 1\n",
    "texts_reduced = texts[0::k]\n",
    "labels_reduced = labels[0::k]\n",
    "\n",
    "print('Number of documents:', len(texts_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd53e33",
   "metadata": {},
   "source": [
    "Use the function ```train_test_split```from ```sklearn``` function to set aside test data that you will use during the lab. Make it one fifth of the data you have currently.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5be9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "texts_reduced, test_texts, labels_reduced, test_labels = train_test_split(texts_reduced, labels_reduced, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbce6d4",
   "metadata": {},
   "source": [
    "## 1 - Document Preprocessing\n",
    "\n",
    "You should use a pre-processing function you can apply to the raw text before any other processing (*i.e*, tokenization and obtaining representations). Some pre-processing can also be tied with the tokenization (*i.e*, removing stop words). Complete the following function, using the appropriate ```nltk``` tools. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e076e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632db6f",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32124cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lmcastanedame\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b33d4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecte d'interieur/designer\n",
      "10\n",
      "architecte interieur designer\n"
     ]
    }
   ],
   "source": [
    "# Look at the data and apply the appropriate pre-processing\n",
    "print(texts_reduced[0])\n",
    "print(labels_reduced[0])\n",
    "\n",
    "# Lowercase\n",
    "texts_reduced = [text.lower() for text in texts_reduced]\n",
    "\n",
    "# Remove punctuation\n",
    "texts_reduced = [re.sub(r'[^\\w\\s]', ' ', text) for text in texts_reduced]\n",
    "\n",
    "# Tokenize\n",
    "texts_reduced = [word_tokenize(text) for text in texts_reduced]\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('french'))\n",
    "texts_reduced = [[word for word in text if word not in stop_words] for text in texts_reduced]\n",
    "\n",
    "# Lemmatize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "texts_reduced = [[lemmatizer.lemmatize(word) for word in text] for text in texts_reduced]\n",
    "\n",
    "# Re-join the words\n",
    "texts_reduced = [' '.join(text) for text in texts_reduced]\n",
    "\n",
    "print(texts_reduced[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4140f1",
   "metadata": {},
   "source": [
    "Now that the data is cleaned, the first step we will follow is to pick a common vocabulary that we will use for every representations we obtain in this lab. **Use the code of the previous lab to create a vocabulary.**\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6910a7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcb7810f",
   "metadata": {},
   "source": [
    "What do you think is the **appropriate vocabulary size here** ? Would any further pre-processing make sense ? Motivate your answer.\n",
    "\n",
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0ba21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260f7b7d",
   "metadata": {},
   "source": [
    "## 2 - Symbolic text representations\n",
    "\n",
    "We can use the ```CountVectorizer``` class from scikit-learn to obtain the first set of representations:\n",
    "- Use the appropriate argument to get your own vocabulary\n",
    "- Fit the vectorizer on your training data, transform your test data\n",
    "- Create a ```LogisticRegression``` model and train it with these representations. Display the confusion matrix using functions from ```sklearn.metrics``` \n",
    "\n",
    "Then, re-execute the same pipeline with the ```TfidfVectorizer```.\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1601c205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c33e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3bcf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ed387bc",
   "metadata": {},
   "source": [
    "## 3 - Dense Representations from Topic Modeling\n",
    "\n",
    "Now, the goal is to re-use the bag-of-words representations we obtained earlier - but reduce their dimension through a **topic model**. Note that this allows to obtain reduced **document representations**, which we can again use directly to perform classification.\n",
    "- Do this with two models: ```TruncatedSVD``` and ```LatentDirichletAllocation```\n",
    "- Pick $300$ as the dimensionality of the latent representation (*i.e*, the number of topics)\n",
    "\n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4204cc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a4eafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b05616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4893c948",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "We picked $300$ as number of topics. What would be the procedure to follow if we wanted to choose this hyperparameter through the data ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c105a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60f74d75",
   "metadata": {},
   "source": [
    "## 4 - Dense Count-based Representations\n",
    "\n",
    "The following function allows to obtain very large-dimensional vectors for **words**. We will now follow a different procedure:\n",
    "- Step 1: Obtain the co-occurence matrix, based on the vocabulary, giving you a vector by word in the vocabulary.\n",
    "- Step 2: Apply an SVD to obtain **word embeddings** of dimension $300$, for each word in the vocabulary.\n",
    "- Step 3: Obtain document representations by aggregating embeddings associated to each word in the document.\n",
    "- Step 4: Train a classifier on the (document representations, label) pairs. \n",
    "\n",
    "Some instructions:\n",
    "- In step 1, use the ```co_occurence_matrix``` function, which you need to complete.\n",
    "- In step 2, use ```TruncatedSVD```to obtain word representations of dimension $300$ from the output of the ```co_occurence_matrix``` function.\n",
    "- In step 3, use the ```sentence_representations``` function, which you will need to complete.\n",
    "- In step 4, put the pipeline together by obtaining document representations for both training and testing data. Careful: the word embeddings must come from the *training data co-occurence matrix* only.\n",
    "\n",
    "Lastly, add a **Step 1b**: transform the co-occurence matrix into the PPMI matrix, and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ed10f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurence_matrix(corpus, vocabulary, window=0):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        corpus (list of list of strings): corpus of sentences\n",
    "        vocabulary (dictionary): words to use in the matrix\n",
    "        window (int): size of the context window; when 0, the context is the whole sentence\n",
    "    Returns:\n",
    "        matrix (array of size (len(vocabulary), len(vocabulary))): the co-oc matrix, using the same ordering as the vocabulary given in input    \n",
    "    \"\"\" \n",
    "    l = len(vocabulary)\n",
    "    M = np.zeros((l,l))\n",
    "    for sent in corpus:\n",
    "        # Get the sentence\n",
    "        sent = ...\n",
    "        # Obtain the indexes of the words in the sentence from the vocabulary \n",
    "        sent_idx = ...\n",
    "        # Avoid one-word sentences - can create issues in normalization:\n",
    "        if len(sent_idx) == 1:\n",
    "                sent_idx.append(len(vocabulary)-1) # This adds an Unkown word to the sentence\n",
    "        # Go through the indexes and add 1 / dist(i,j) to M[i,j] if words of index i and j appear in the same window\n",
    "        for i, idx in enumerate(sent_idx):\n",
    "            # If we consider a limited context:\n",
    "            if window > 0:\n",
    "                # Create a list containing the indexes that are on the left of the current index 'idx_i'\n",
    "                l_ctx_idx = ...                \n",
    "            # If the context is the entire document:\n",
    "            else:\n",
    "                # The list containing the left context is easier to create\n",
    "                l_ctx_idx = ...\n",
    "            # Go through the list and update M[i,j] and M[j,i]:        \n",
    "            for j, ctx_idx in enumerate(l_ctx_idx):\n",
    "                ...\n",
    "                ...\n",
    "    return M  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be497b3",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e843cc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the co-occurence matrix, transform it as needed, reduce its dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58160c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b233d5ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b312da12",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_representations(texts, vocabulary, embeddings, np_func=np.mean):\n",
    "    \"\"\"\n",
    "    Represent the sentences as a combination of the vector of its words.\n",
    "    Parameters\n",
    "    ----------\n",
    "    texts : a list of sentences   \n",
    "    vocabulary : dict\n",
    "        From words to indexes of vector.\n",
    "    embeddings : Matrix containing word representations\n",
    "    np_func : function (default: np.sum)\n",
    "        A numpy matrix operation that can be applied columnwise, \n",
    "        like `np.mean`, `np.sum`, or `np.prod`. \n",
    "    Returns\n",
    "    -------\n",
    "    np.array, dimension `(len(texts), embeddings.shape[1])`            \n",
    "    \"\"\"\n",
    "    representations = []\n",
    "    for text in texts:\n",
    "        indexes = ... # Indexes of words in the sentence obtained thanks to the vocabulary\n",
    "        sentrep = ... # Embeddings of words in the sentence, aggregated thanks to the function\n",
    "        representations.append(sentrep)\n",
    "    representations = np.array(representations)    \n",
    "    return representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462a0a2",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5256e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain document representations, apply the classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7832550e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a9dbec5",
   "metadata": {},
   "source": [
    "## 5 - Dense Prediction-based Representations\n",
    "\n",
    "We will now use word embeddings from ```Word2Vec```: which we will train ourselves\n",
    "\n",
    "We will use the ```gensim``` library for its implementation of word2vec in python. Since we want to keep the same vocabulary as before: we'll first create the model, then re-use the vocabulary we generated above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80339bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2383776e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(vector_size=300,\n",
    "                 window=5,\n",
    "                 null_word=len(word_counts))\n",
    "model.build_vocab_from_freq(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b48e7ec",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895599f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model is to be trained with a list of tokenized sentences, containing the full training dataset.\n",
    "preprocessed_corpus = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ba2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(preprocessed_corpus, total_examples=..., epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e1daa",
   "metadata": {},
   "source": [
    "Then, we can re-use the ```sentence_representations```function like before to obtain document representations, and apply classification. \n",
    "<div class='alert alert-block alert-info'>\n",
    "            Code:</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66123a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a18b961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b21842b8",
   "metadata": {},
   "source": [
    "<div class='alert alert-block alert-warning'>\n",
    "            Question:</div>\n",
    "            \n",
    "Comment on the results. What is the big issue with the dataset that using embeddings did not solve ? \n",
    "**Given this type of data**, what would you propose if you needed solve this task (i.e, reach a reasonnable performance) in an industrial context ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c778f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
